<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>mbyim</title>
 <link href="http://mbyim.com/atom.xml" rel="self"/>
 <link href="http://mbyim.com/"/>
 <updated>2024-01-11T22:29:33-05:00</updated>
 <id>http://mbyim.com</id>
 <author>
   <name>Martin Yim</name>
   <email></email>
 </author>

 
 <entry>
   <title>Synthetic Control with Goodreads and the Financial Times</title>
   <link href="http://mbyim.com/2024/01/11/Synthetic-Control-Goodreads-FT/"/>
   <updated>2024-01-11T00:00:00-05:00</updated>
   <id>http://mbyim.com/2024/01/11/Synthetic-Control-Goodreads-FT</id>
   <content type="html">&lt;p&gt;Every year, various outlets put out &quot;Best of Year&quot; articles for
various things. In this case, the &lt;em&gt;Financial Times&lt;/em&gt; put out &lt;a href=&quot;https://www.ft.com/booksof2023&quot;&gt;Best Books
of 2023&lt;/a&gt;, for several
subjects. I was curious what the effect would be on book sales –
especially because some of these books are more academic or obscure on
the whole. I don&apos;t have access to book sales data, but Goodreads does
have statistics on when people &quot;add&quot; books to their &quot;shelves&quot;. Using
this publically available Goodreads data as a proxy for interest and
marketing effectiveness, we can try to estimate if the &lt;em&gt;Financial Times&lt;/em&gt;
series had a significant effect on interest in the books featured (and
thus, presumably also book sales).&lt;/p&gt;

&lt;p&gt;I made a few assumptions and caveats in this quick analysis. One
assumption was that two different goodreads action statistics could be
additive (&quot;added&quot; and &quot;to-read&quot;). Another was that I could try to
use the other books of authors&apos; whose books were featured in the &lt;em&gt;Best
Of&lt;/em&gt; series as a donor pool to create synthetic controls. Strong spikes
for different books at different times outside of the &lt;em&gt;FT Best Of&lt;/em&gt;
publication dates I assume to be other, &quot;exogenous&quot;
marketing/publicity efforts. Lastly, while the &lt;em&gt;FT Best Of&lt;/em&gt; series
contained many subject list &lt;em&gt;Best Of&lt;/em&gt;s, I scoped the analysis to
Economics, History, and Politics, for ease and to restrict my
web-scraping volume.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regarding methodology, I chose &lt;a href=&quot;https://en.wikipedia.org/wiki/Synthetic_control_method&quot;&gt;synthetic
control&lt;/a&gt;
to try to estimate the effect. The gist of synthetic control is to
construct a counterfactual from donor units which then allows us to peer
into the un-seen, untreated outcomes of our treated units. It does this
using regression to find the weights which then are applied to the donor
units to properly mimic the treated unit&apos;s would-have-been future. The
method is fairly recent and gaining traction in the social science
literature.&lt;/p&gt;

&lt;p&gt;There are a few reasons I chose synthetic control over other possible
methods. First, marketing mix models would be one possibility but they
are generally done internally where all marketing decision data is
available because they are effective when one has access to the full
data on marketing efforts – how &quot;strong&quot; those efforts were in dollar
terms, when they started and stopped, etc – which we don&apos;t have.
Second, after doing some reading on regression discontinuity in time, I
decided it had issues that were better addressed by synthetic control.
Synthetic control would better be able to deal with time-varying
unobservable variables. As said in &lt;a href=&quot;https://www.nber.org/system/files/working_papers/w23602/w23602.pdf&quot;&gt;this
paper&lt;/a&gt;:
&quot;RDiT requires assumptions for identification that are often strong and
inherently untestable.&quot; While my application of synthetic control and
definition of the donor pool here are imperfect, I prefer it to RDiT –
and one might even argue that a simple pre-post analysis might also
suffice for most situations (as they do in the aforementioned paper).
But we&apos;re just having fun here, and I wanted to see what I could do
with synthetic control :).&lt;/p&gt;

&lt;p&gt;To view the dataset from above we can use panelView (authored in part by
the same people who worked on gsynth, which we will introduce in a
moment). As we can see we have about 40 treated books, and roughly 250
untreated as our donor pool. We can also see the (slightly) staggered
treatment. We also have a significant size in terms of panel data,
covering a little over five months.&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;/assets/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At first, borrowing heavily from code in &lt;a href=&quot;https://matheusfacure.github.io/python-causality-handbook/15-Synthetic-Control.html&quot;&gt;Causal Inference for the
Brave and
True&lt;/a&gt;,
I created a simple synthetic control model using our treated and
untreated books. I excluded about 10 books (treated and untreated) which
had very high pretreatment spikes, as these reflected exogenous
variation of other marketing efforts and which seemed to be heavily
biasing the model (see below). Outside of these outliers, I had two main
difficulties to deal with: one was that I had more than one treated unit
(about 40 in my case), and the other was that I realized after a bit
that the &lt;em&gt;Best Of&lt;/em&gt; series was &lt;em&gt;not&lt;/em&gt; all published on one day, so I
actually had three different treatment periods as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To deal with the issue of multiple treatment units I first attempted to
create synthetic controls for each treated unit and add the synthetic
control outcomes together. This model passed the smell test but it felt
a bit crude; I wanted to see what would happen if I collapsed the
treatment units into one, with an average. In the second attempt, I
aggregated the treated books into a single treated unit, and this time I
came up with a model that passed the smell test. However, after reading
&lt;a href=&quot;https://stats.stackexchange.com/questions/616939/synthetic-control-method-based-on-several-treated-units&quot;&gt;this stackexchange
post&lt;/a&gt;,
I decided to make use of the gsynth package which could help me directly
address both my problems of multiple treated units and staggered
treatments with a proper methodology, as opposed to my jerry-rigged
solutions where I wasn&apos;t even addressing the staggered treatments (only
staggered one to two days).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The gsynth package implements &lt;a href=&quot;https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3&quot;&gt;Generalized Synthetic
Control&lt;/a&gt;,
which implements a variation on synthetic control using two way fixed
effects and a factor loading scheme. Importantly, it is able to
incorporate multiple treated units and staggered treatments. I should
note that it does have a trade off of interpretability: where synthetic
control restricts weights of its constituent donors to add to 1 and be
positive, generalized synthetic control is calculating factors and also
does not necessarily constrain them to add to 1 and all take on positive
values. As said in the paper: &quot;estimated factors may not be directly
interpretable because they are, at best, linear transformations of the
true factors...&quot;&lt;/p&gt;

&lt;p&gt;After fitting the gysnth model, we can see that the overall results are
similar to the averaged out book model I created in python which didn&apos;t
properly address the multiple treatment unit or staggered treatment.
However the gsynth model here performs better &lt;em&gt;despite&lt;/em&gt; the fact that I
did not exclude the outliers I mentioned above, which had severely
thrown off the original model (see
below).&lt;img src=&quot;/assets/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To give a singular example of the results for a single treatment unit,
below are three graphs of Branko Milanovic&apos;s new book &lt;a href=&quot;https://www.goodreads.com/book/stats?id=123831773&quot;&gt;&lt;em&gt;[Visions of
Inequality]{.underline}&lt;/em&gt;&lt;/a&gt;,
the goodreads data (with a dotted line added for treatment date), the
actual versus the blue dotted synthetic control, and the gap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Critically we can see that the model does not overfit, but also that it
does seem to be able to isolate the FT effect, even with the (perhaps
janky) definition of a donor pool that we used in this analysis.
Eyeballing, it seems as though the ATT really occurs over one to two
weeks before trailing off into residual noise. Though perhaps a simple
before-after eye test might have sufficed, I think this is a bit more
fun, and it was interesting to see how synthetic control would work. You
can find some of the (messy, quick – please excuse it) webscraping and
modeling code &lt;a href=&quot;https://github.com/mbyim/goodreads_scraping&quot;&gt;here&lt;/a&gt; if you&apos;re interested.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Meta Is Better: Musings on RPA</title>
   <link href="http://mbyim.com/2020/02/10/Meta-Is-Better-RPA/"/>
   <updated>2020-02-10T00:00:00-05:00</updated>
   <id>http://mbyim.com/2020/02/10/Meta-Is-Better-RPA</id>
   <content type="html">&lt;p&gt;&lt;em&gt;“Worse is better” - Ancient Proverb&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Admittedly, I only heard about RPA recently while browsing around, but it caught my eye. Companies like &lt;a href=&quot;https://www.uipath.com/&quot;&gt;UiPath&lt;/a&gt; have experienced explosive &lt;a href=&quot;https://techcrunch.com/2019/06/24/gartner-finds-rpa-is-fastest-growing-market-in-enterprise-software/&quot;&gt;growth&lt;/a&gt;. And there is reason to believe that the time is ripe for “Robotic Process Automation” solutions. But why now, and what makes this automation the sort that actually happens and isn’t perpetually on the horizon?&lt;/p&gt;

&lt;p&gt;RPA is living up to its promise because it is a &lt;em&gt;metainterface&lt;/em&gt;: it creates new opportunity for automation through increasing interoperability of systems. Software is now old enough as an industry tool that clunky out-dated legacy systems and external systems without good application programming interfaces lurk in the spines of many a organization, creating procedural and operational drag in a variety of firm functions. Legacy systems or external systems that don’t have welcoming API’s with which to conduct scalable operations make for higher cost curves to what automation opportunities are available, limiting what is actually worth automating. As a result, it is difficult or expensive to automate processes in settings where not all (or any) pieces of the flow are using software applications with APIs. A manual process may start from legacy software with only a GUI, to another application with an API available if needed, and back to GUI-only legacy. Meta is better. By making an interface for your interfaces – effectively by making all interfaces programmable – RPA enables proper automation for procedures that would have been unfeasible, cumbersome, or too costly to perform before.&lt;/p&gt;

&lt;p&gt;Changing cost curves for automating processes means changing the firm’s internal dynamics. By using programming-by-demonstration and drag-and-drop paradigms in tools like “UiPath Studio”, making RPA workflows becomes a cross between using an Adobe product and more raw forms of programming like VBA and Python. Tools designed like UiPath Studio enable non- or semi- technical users to transition into RPA development much more quickly than if they were to attempt to transition into software engineers, which in turn, means that companies can quickly adopt RPA internally via some training and deployment and begin reaping benefits rapidly. Additionally, RPA is often even leveraged to offload work from engineering departments where development time can be highly valued as a means of delivering on the core value proposition of a software product. Instead of having scarce engineering resources expended helping to create some internal tooling, often RPA can be an alternative solution which can be quickly and cheaply (in terms of firm resources) developed and deployed.&lt;/p&gt;

&lt;p&gt;In essence: RPA changes the calculation of what processes are worthy of consideration for automation because it enables interoperability between systems with lower costs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rpa.png&quot; alt=&quot;Histogram of Business Process Automation Reward/Cost Ratio&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One initial skepticism I had about RPA was what is called Process Discovery: how to identify processes that are high reward/cost ratio? And how costly or difficult would the actual process discovery be in practice – especially in larger and older organizations? It turns out that with sufficient initial support from the Top (where the imitative for RPA seems to often times originate, anyways) this isn’t particularly difficult for a firm with some dedication to the task. And - perhaps in an appropriate twist - UiPath has already started developing a &lt;a href=&quot;https://www.uipath.com/product/process-understanding-explorer&quot;&gt;tool&lt;/a&gt; to help automate the process of discovering processes that are good candidates for automation.&lt;/p&gt;

&lt;p&gt;There are some downsides to RPA as one might expect. For example: RPA processes can be fragile and prone to depreciating due to the underlying applications/interfaces evolving – which especially the case with UI/UX redesigns from external systems that are part of a workflow (its probably less common for APIs to be depreciated or changed). Another difficulty is simply that many business processes contain large numbers of possible exceptions that can occur at any given process step that need to be navigated: this the kind of thing that can be annoying or cumbersome to deal with and can take more time and effort to automate end-to-end, which can limit RPA’s effectiveness in some areas. However, even here, &lt;a href=&quot;https://www.youtube.com/watch?v=yoxz_DV0BIU&quot;&gt;human-in-the-loop&lt;/a&gt; approaches are being leveraged here to bring automation capabilities right to the edge. RPA – or automation, in general – isn’t solely about cost reduction, but simultaneously augmenting productivity.&lt;/p&gt;

&lt;p&gt;Depending on your point of view, RPA can seem like a clever and elegant solution to a broad class of problems, or a glossy hack over the woeful state of software and tools. Nevertheless the space is growing, alongside UiPath, Automation Anywhere and others, there are tools like Zapier and automate.io, and even android apps like Automate (which I’ve heard are sometimes used in click farms).&lt;/p&gt;

&lt;p&gt;Software eating the world didn’t mean that interoperability would be a free lunch. But now, at least, you can pay for it.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Implementing Turchin's Metaethnic Frontier Theory With NetLogo</title>
   <link href="http://mbyim.com/2019/12/03/Metaethnic-Frontier-Implementation/"/>
   <updated>2019-12-03T00:00:00-05:00</updated>
   <id>http://mbyim.com/2019/12/03/Metaethnic-Frontier-Implementation</id>
   <content type="html">&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;

&lt;p&gt;Peter Turchin is a Quantitative Biologist turned Quantitative Historian. In his approach to history he attempts to capture historical trends and patterns by leveraging data on expired agrarian societies and even adjusting his theories to attempt to &lt;a href=&quot;https://indieocean.io/martin/ages-of-discord&quot;&gt;encompass more modern industrial societies&lt;/a&gt;. Building on work by Jack Goldstone, Professor Turchin has proposed various models to explain “historical dynamics” as part of the study he calls Cliodynamics. In one of his early books on Cliodynamics, &lt;a href=&quot;https://indieocean.io/martin/historical-dynamics&quot;&gt;Historical Dynamics&lt;/a&gt;, Turchin lays out details of varous models he uses to try and understand how societies function and evolve. The Metaethnic Frontier theory is proposed to model the geopolitical dynamics of competing polities, with special attention paid to the sociological concept of Asabiya and several assumptions based on previous work. I decided it would be a fun way to try and learn more about agent-based modeling by implementing in NetLogo, a popular piece of software for Agent-Based modeling and simulation.&lt;/p&gt;

&lt;h3 id=&quot;netlogo&quot;&gt;NetLogo&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ccl.northwestern.edu/netlogo/&quot;&gt;NetLogo&lt;/a&gt; is an environment for doing agent-based modeling and simulation. The general framework is that the “observer” manipulates “turtles” (agents), “patches” (space/territory), and “links” (relationships between turtles or patches to each other) through time (“ticks”). The GUI provides graphic representations of the model being built and run, as well as providing ways to easily plot data generated by the model, create buttons to “setup” or run (“go”) the model, and other features. The NetLogo language itself is a lisp-y agent-based language that can take some getting used to as it contains some unique concepts like “self” and “myself”. The NetLogo website also has a &lt;a href=&quot;https://ccl.northwestern.edu/netlogo/models/&quot;&gt;library&lt;/a&gt; of example models that are often helpful for understanding how to implement certain concepts.&lt;/p&gt;

&lt;h3 id=&quot;the-theory&quot;&gt;The Theory&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/?title=Asabiya&amp;amp;redirect=yes&quot;&gt;Asabiya&lt;/a&gt; is a concept from the writings of Ibn Khaldun which Turchin defines as “the capacity for collective action” of a society. The Metaethnic Frontier theory is meant to incorporate asabiya as a key factor in predicting the dynamics of imperial agrarian societies - how they grow, shrink, and begin. Turchin posits that &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_selection#Multilevel_selection_theory&quot;&gt;multi-level selection&lt;/a&gt; can help us identify the dynamics of asabiya in groups. He follows by noting three ways in which the logic of multi-level selection can be relevant in understanding change in “collective solidarity”: intergroup conflict, population and resource constraints, and ethnic boundaries.&lt;/p&gt;

&lt;p&gt;For small groups, intergroup conflict can increase asabiya as people need to band together to survive as a group. Conversely (again for small groups), a large population with respect to available resources can decrease asabiya as individuals compete for limited resources. For larger groups, Turchin proposes that ethnic boundries can influence how bands of small groups with moderate ethnic differences can band together against people who are even more “ethnically distanced” - more “Other”. In this process of small groups banding together against peoples more Other than themselves, they can form what Turchin calls a &lt;em&gt;Metaethnic Frontier&lt;/em&gt;, which manifests in a more symbolic form (eg: exclusionary religion). Turchin notes that the this ethnic boundry dynamic which generates asabiya in a large group (composed of smaller groups) is weak because as the size of the group grows larger, the central regions are less exposed to intergroup conflict and asabiya decreases, leading to greater internal division. Finally, Turchin notes that all three aforementioned possiblities occur at regions which constitute imperial and metaethnic frontiers (imperial and metaethnic frontiers often coincide, he notes). Its is in these regions of intense dynamics where asabiya is forged which are most prone to &lt;em&gt;ethnogenesis&lt;/em&gt; - where new ethnic identites are formed, and new empires are seeded.&lt;/p&gt;

&lt;h3 id=&quot;the-model&quot;&gt;The Model&lt;/h3&gt;

&lt;p&gt;The model is predicated on a straightforward set of equations that dictate how values change over the discrete time of the model. We presume a matrix of cells (“patches”) where each cell is essentially a regional polity - either as part of the “non-imperial hinterland” or as a piece of an empire. Each cell (regional polity) takes on a regional asabiya, and an imperial asabiya (for hinterland regions I set these two variables to be the same value). In addition, each cell is assigned an imperial index (0 for hinterland, and some integer for a cell belonging to an empire). I also gave a unique color for each imperial index. Likewise, cells are also assigned some other variables used for calculations, such as the imperial “center of gravity”, average imperial asabiya, and number of regions constituting each empire.&lt;/p&gt;

&lt;p&gt;The regional (cell) asabiya value over time steps (“ticks” in NetLogo) are determined as follows. If the region has a neighbor (not including diagonally adjacent cells) which is of a different imperial index, then asabiya increases in accordance with the following formula:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/png.latex?S_{x,y,t+1}=S_{x,y,t}+r_0S_{x,y,t}(1-S_{x,y,t}) &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, r_0 signifies some defined parameter. Indices x, y, and t represent x and y coordinates of the cell in the matrix and the time t in ticks, respectively. In the other case, if the region in question is neighbors only with cells of the same imperial index, asabiya decreases according to the following formula:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/png.latex?S_{x,y,t+1}=S_{x,y,t}-$\delta$S_{x,y,t}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, delta signifies some defined parameter/constant. These parameters are set as global variables in the model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note: I may have differed from the model defined in the book in this respect. I allowed for hinterland regions to attack other 0-indexed regions and possibly conquer them. In addition, I randomly assigned their initial regional/imperial asabiya value as opposed to setting them all equal to 0. This is something I might tweak in a second pass.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Calculating the imperial asabiya at each time step, we simply take the average of the region asabiya of all cells with the respective imperial index:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/png.latex?\bar{S}_{i,t}=\dfrac{1}{A_{i,t}}\sum\limits_{\{x,y\}\in\ i}S_{x,y,t}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, A signifies number of regions an empire i controls at time step, t. We will now use the imperial asabiya to help calculate a definition of a region’s power, which indicates how well it can attack or defend itself. Cells attack by randomly choosing a neighboring cell with a differing imperial index. Below is the region/cell’s power definition needed for calculating outcomes when cells decide to attack each other:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/png.latex?P_{x,y,t}={A_{i,t}}\bar{S}_{i,t}\exp\[-d_{i,x,t}/h]&quot; /&gt;&lt;/p&gt;

&lt;p&gt;d here represents the distance between the cell in question and the average of the x and y coordinates of the empire’s cells. This average location is the “center of gravity” of the empire. h signifies how well imperial power travels over distance. The significance here is that as distance between the imperial center of gravity and the regional cell grows larger, it mitigates the power the cell. Likewise, modulating h affects how much distance plays a role in the polity power calculation. Next, a cell wins an attack and takes over the defending cell (change of imperial index) when the following is true:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/png.latex?P_{att}-P_{def}\textgreater{\Delta}_{p}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Delta_p denotes a parameter for power differential necessary to declare an attack successful.&lt;/p&gt;

&lt;p&gt;Finally, at the end of a time step, we calculate whether an empire collapses due to falling below a critical imperial asabiya threshold. If this occurs, the empire’s cells are changed to be non-imperial hinterland.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/png.latex?\bar{S}_{i,t}\textgreater S_{crit}&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-implementation&quot;&gt;The Implementation&lt;/h3&gt;
&lt;p&gt;After implementing this in NetLogo, I saw many of the dynamics Turchin noted, as well as instances where my implementation might have deviated in important ways. One important difference I should note is the beginning. I do as Turchin specifies in starting out with a set of four neighboring polities which comprise of the first empire, but in many cases, it is immediately destroyed as in my implementation. As I noted above, I believe this is because I am currently not initializing all hinterland region with 0 asabiyah from the beginning but randomizing their asabiyah such that they will in fact immediately begin to conquer each other. As a result you can see an initial cambrian explosion of small empires and a subsequent consolidation stage where a handful of empires make it out of the initial chaos alive.&lt;/p&gt;

&lt;p&gt;To begin with, each NetLogo model requires two procedures: a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup&lt;/code&gt; procedure and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go&lt;/code&gt; procedure, which initialize and execute the model. Below is the code for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go&lt;/code&gt; procedure.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;;Algorithm for time steps
to go
  let empires (remove-duplicates [imperialindex] of turtles)

  update-asabiya

  ask turtles [
    calculate-imperial-asabiya
    calculate-imperial-num-regions-controlled
    calculate-imperial-center
    calculate-turtle-power
  ]

  ;attacks
  decide-attack

  ;at end of each time period, check for imperial collapse
  calc-imperial-collapse

  tick
end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go&lt;/code&gt; procedure we see that we can execute procedures such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-asabiya&lt;/code&gt; as well as other commands. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ask turtles&lt;/code&gt; command here is a function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ask&lt;/code&gt; which takes two arguments: a) an agent or agentset (turtles) and b) a list of commands. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;let&lt;/code&gt; here is allowing us to define a local variable that will be recalculated each timestep and is used in another computation in this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go&lt;/code&gt; procedure. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tick&lt;/code&gt; is updating the model to start the next discrete time block.&lt;/p&gt;

&lt;p&gt;Notice that I don’t do all of the procedures in one &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ask turtles&lt;/code&gt; command. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ask turtles&lt;/code&gt; command works by going through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;agentset&lt;/code&gt; - turtles (a cell/polity) in this case - and running all commands in the command list, one by one - not concurrently. With update-asabiya, for example, I want to calculate regional asabiya for all turtles &lt;strong&gt;before&lt;/strong&gt; any turtle does any other actions or calculations in the timestep because some of them use the updated regional asabiya. Therefore I isolate that computation in its own &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ask turtles&lt;/code&gt; block and procedure, seen here:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;to update-asabiya
ask turtles [
  ifelse any? ( turtles-on neighbors4 ) with [ imperialindex = 0 or imperialindex != [imperialindex] of myself ]
  ;let tminusoneasabiya asabiya
  ; Sx,y,t+1 = Sx,y,t + r0Sx,y,t(1-Sx,y,t)
  [ set asabiya (asabiya + r-o * asabiya * (1 - asabiya) ) ]
  ; Sx,y,t+1 = Sx,y,t - dSx,y,t
  [ set asabiya (asabiya - delta * asabiya ) ]
  ]
end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Much of the NetLogo code looks more or less like the above - procedures defining computations against certain agentsets, moving forward in time, setting variables, etc. Now, if we run the model, we can see the initial growth and then consolidation of polities (which is likely not how Turchin’s model in the beginning stages would look, if followed to the tee.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/first_phase.gif&quot; alt=&quot;Beginnings&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once there is a more conslidated set of polities - while still having significant evolution - we can see a great example of a new state appearing at the intersection of some frontiers and expanding. On the right, the light purple empire emerges out of the frontier region between two other large empires. Ethnogenesis in action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ethnogenesis.gif&quot; alt=&quot;Ethnogenesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we can witness some empire collpase when a few empires’ collective imperial asabiya falls below the critical point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/collapse.gif&quot; alt=&quot;Empire Collapse&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also made some graphs showing polity size and imperial asabiya over time. As expected, as time goes on and the surviving empires become larger regions, the imperial asabiya descreases due to the low asabiya of the central regions of said empires. Likewise, we can see the average size of the polities increase over time in this first cycle, at least.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/netlogo_graphing.png&quot; alt=&quot;NetLogo Graphing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To generate gifs from the NetLogo model run, I used the &lt;a href=&quot;https://stackoverflow.com/a/26714353&quot;&gt;export-view function&lt;/a&gt; in conjunction with &lt;a href=&quot;https://stackoverflow.com/a/54138795&quot;&gt;this&lt;/a&gt; imagemagick snippet which sorts the images by filename when creating the gif&lt;/p&gt;

&lt;h3 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h3&gt;

&lt;p&gt;NetLogo was an interesting experience, but I’m sure I haven’t mastered it yet. As this was my first attempt at using NetLogo, I found myself at a bit of a loss at how to implement this particular model and concepts like “self” vs “myself” were quite foreign. I was able to get by without needing to use “links” so I still know very little about them. The basic thrust of the logic of my implementation in NetLogo was to make each patch host exactly one turtle, where I then just dealt with turtles instead of patches. Now looking back, I suspect you should be able to do with just with patches, avoiding the extra complexity. I may revisit that in the future. In general I was a bit unsure of how idiomatic my code was for this, and I felt that I had hacked together some parts in unelegant ways (eg: chosing a non-used unique color for a new empire). I’m actually not sure whether my implementation is entirely faithful to the original model, as a few details were not entirely clear - and I may have also made some mistakes on this first pass. Nonetheless, I was able to see some of the same dynamics that Turchin describes in the book and the general thrust still seemed to hold quite well. You can see the NetLogo model code &lt;a href=&quot;https://github.com/mbyim/turchin_metaethnic_frontier_theory&quot;&gt;here&lt;/a&gt;. If you have any feedback, questions, or comments about the code or otherwise, do reach out.&lt;/p&gt;

&lt;p&gt;If you’re still curious, another explanation/implementation of the Metaethnic Frontier theory (in unity) can be found in &lt;a href=&quot;https://www.paulkanyuk.com/cliodynamics&quot;&gt;this&lt;/a&gt; blog post.&lt;/p&gt;
</content>
 </entry>
 

</feed>
